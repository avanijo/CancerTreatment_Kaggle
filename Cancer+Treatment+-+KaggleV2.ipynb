{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective:** Develop algorithms to classify genetic mutations into different classes based on clinical evidence (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with first exploring our data files - training_variants and training_text. Data exploration will help to decide the path for model building more accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries \n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob as tb\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gensim\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now read all the data files as Pandas dataframes and print their dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we will take a look at our data summary\n",
    "\n",
    "train_variants_df = pd.read_csv(\"C:/Users/AJ186043/Desktop/WORK/Kaggle Competitions/Cancer Treatment/Dataset/training_variants\")\n",
    "test_variants_df = pd.read_csv(\"C:/Users/AJ186043/Desktop/WORK/Kaggle Competitions/Cancer Treatment/Dataset/test_variants\")\n",
    "train_text_df = pd.read_csv(\"C:/Users/AJ186043/Desktop/WORK/Kaggle Competitions/Cancer Treatment/Dataset/training_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\n",
    "test_text_df = pd.read_csv(\"C:/Users/AJ186043/Desktop/WORK/Kaggle Competitions/Cancer Treatment/Dataset/test_text\", sep=\"\\|\\|\", engine=\"python\", skiprows=1, names=[\"ID\", \"Text\"])\n",
    "print(\"Train Variant\".ljust(15), train_variants_df.shape)\n",
    "print(\"Train Text\".ljust(15), train_text_df.shape)\n",
    "print(\"Test Variant\".ljust(15), test_variants_df.shape)\n",
    "print(\"Test Text\".ljust(15), test_text_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see the columns present in both training_variants and training_text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_variants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we come to know that both our files have a common column i.e. ID column. Other than this the training_variants file has 4 columns - ID, Gene, Variation and class respectively. While trainin_text file had ID and Text columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_variants_df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the next step we are going to calculate the unique count for all the columns in the training_variants file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will calculate the count of unique values in training_variants file using unique() function\n",
    "\n",
    "print(\"For training data, there are a total of\", len(train_variants_df.ID.unique()), \"IDs,\", end='')\n",
    "print(len(train_variants_df.Gene.unique()), \"unique genes,\", end='')\n",
    "print(len(train_variants_df.Variation.unique()), \"unique variations and \", end='')\n",
    "print(len(train_variants_df.Class.unique()),  \"classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get that we have 9 unique classes in which the data needs to be categorized. Let us calculate the frequency for all these 9 classes to better understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the frequency for each class\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.countplot(x=\"Class\", data=train_variants_df)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Class Count', fontsize=12)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Frequency of Classes\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows that class 7 has the highest frequency, followed by class 4, class 1 and so on. Class 8 has the least frequency.\n",
    "Now we are going to see the count for each Gene type present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see the number of times a particular gene occures in our data\n",
    "\n",
    "gene_group = train_variants_df.groupby(\"Gene\")['Gene'].count()\n",
    "max_occ_genes = gene_group.sort_values(ascending=False)\n",
    "print(\"Genes with maximal occurences\\n\", gene_group.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the Gene type BRCA1 has the highest count of occurence. We will now take out the top 10 genes (on the basis of their occurence count) and plot them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_occ_top = max_occ_genes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_occ_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the top 10 genes according to count \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "max_occ_top.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the above graph gives us top 10 genes according to their frequencies. We can see that the Gene named BRCA1 has the highest frequency in the overall data followed by TP53, EGFR and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_group = train_variants_df.groupby(\"Class\")['Gene'].count()\n",
    "occ_genes = class_group.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the genes for all 9 classes\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(20,20))\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        gene_count_grp = train_variants_df[train_variants_df[\"Class\"]==((i*3+j)+1)].groupby('Gene')[\"Variation\"].count().reset_index()\n",
    "        sorted_gene_group = gene_count_grp.sort_values('Variation', ascending=False)\n",
    "        sorted_gene_group_top_10 = sorted_gene_group[:10]\n",
    "        sns.barplot(x=\"Gene\", y=\"Variation\", data=sorted_gene_group_top_10, ax=axs[i][j])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we conclude the following :\n",
    "\n",
    "1. BRCA1 is the gene with highest frequency and it appears the most in Class 5\n",
    "2. TP53 is the gene with the second highest frequency and it appears the most in Class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets now explore the training_text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_df.loc[:, 'Text_count']  = train_text_df[\"Text\"].apply(lambda x: len(x.split()))\n",
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = train_variants_df.merge(train_text_df, how=\"inner\", left_on=\"ID\", right_on=\"ID\")\n",
    "train_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_grp = train_full.groupby('Class')[\"Text_count\"]\n",
    "count_grp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full[train_full[\"Text_count\"]==1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_full.drop(train_full[train_full[\"Text_count\"]==1.0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "cloud = WordCloud(width=1440, height=1080).generate(\" \".join(train_full_df[train_full.Class == 7]['Text']))\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "ax = plt.axes()\n",
    "ax.set_title('Class 7 Text Word Cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "cloud = WordCloud(width=1440, height=1080).generate(\" \".join(train_full_df[train_full.Class == 8]['Text']))\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "ax = plt.axes()\n",
    "ax.set_title('Class 8 Text Word Cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf(word, train_full_df):\n",
    "    return blob.words.count(word) / len(blob.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for eliminating Stop Words\n",
    "\n",
    "nltk.download()\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#word_tokenize accepts a string as an input, not a file.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file1 = open(\"train_full_df\")\n",
    "line = file1.read()# Use this to read file content as a stream:\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendFile = open('filteredtext.txt','a')\n",
    "        appendFile.write(\" \"+r)\n",
    "        appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(appendfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data=train_full\n",
    "#stopWords = set(stopwords.words('english'))\n",
    "#words = word_tokenize(train_full)\n",
    "#wordsFiltered = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for w in words:\n",
    " #   if w not in stopWords:\n",
    "  #      wordsFiltered.append(w)\n",
    " \n",
    "#print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_full_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stop words elimination using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "n_features = 50\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=50,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(train_full_df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "tf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In this step we will try to check all the Gene and Variation combinations present in our data\n",
    "train_full_df['Gene_And_Variation']=train_full['Gene']+' '+train_full_df['Variation']\n",
    "train_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check the unique values for Gene+Variation combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full['Gene_And_Variation'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_full.Gene_And_Variation.unique()), \"unique genes,\", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n",
    "    preprocessor=None, stop_words='english', max_features=None)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_of_words = count_vectorizer.fit_transform(train_full['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
